from modal import App, Volume, Image
from diffusers import StableVideoDiffusionPipeline
import torch
from pathlib import Path
import imageio
from PIL import Image as PILImage
import sys

# ---------- é•œåƒä¸ç¼“å­˜ ----------
image = Image.debian_slim().pip_install(
    "diffusers",
    "torch",
    "transformers",
    "imageio",
    "imageio-ffmpeg",
    "Pillow",
    "accelerate"
)

app = App("svd-img2vid")
volume = Volume.from_name("svd-cache", create_if_missing=True)

# ---------- Volume æŒ‚è½½è·¯å¾„ ----------
VOLUME_ROOT = Path("/input")
OUTPUT_FRAMES_DIR = VOLUME_ROOT / "frames"
OUTPUT_VIDEO_DIR = VOLUME_ROOT / "video"
MODEL_CACHE_DIR = VOLUME_ROOT / "model_cache"  # å•ç‹¬çš„æ¨¡å‹ç¼“å­˜ç›®å½•

def ensure_dirs():
    OUTPUT_FRAMES_DIR.mkdir(parents=True, exist_ok=True)
    OUTPUT_VIDEO_DIR.mkdir(parents=True, exist_ok=True)
    MODEL_CACHE_DIR.mkdir(parents=True, exist_ok=True)

def save_frames(frames, base_name="frame"):
    ensure_dirs()
    for idx, frame in enumerate(frames):
        frame_path = OUTPUT_FRAMES_DIR / f"{base_name}_{idx:03d}.png"
        frame.save(frame_path)
    print(f"ğŸ–¼ï¸ å¸§å·²ä¿å­˜è‡³ Volume ä¸­çš„ {OUTPUT_FRAMES_DIR}")

def save_video(frames, output_path=None, fps=5):
    ensure_dirs()
    if output_path is None:
        output_path = OUTPUT_VIDEO_DIR / "output.mp4"
    imgs = [frame.convert("RGB") for frame in frames]
    imageio.mimsave(str(output_path), imgs, fps=fps)
    print(f"ğŸ¥ è§†é¢‘å·²ä¿å­˜è‡³ Volume ä¸­çš„ {output_path}")

def resolve_image_path(image_path: str | None) -> str:
    return str(Path(image_path or "/input/input.png"))

def load_image_pil(path_str: str) -> PILImage.Image:
    img = PILImage.open(path_str).convert("RGB")
    # å›ºå®šåˆ†è¾¨ç‡åˆ°å®˜æ–¹æ¨èå€¼ 1024Ã—576
    return img.resize((1024, 576), PILImage.BICUBIC)

# ---------- å•æ‰¹æ¬¡ç”Ÿæˆ ----------
@app.function(
    gpu="any",
    image=image,
    volumes={"/input": volume},
    timeout=600
)
def generate_video(
    image_path: str = "/input/input.png",
    total_frames: int = 50,
    fps: int = 10
):
    resolved = resolve_image_path(image_path)
    print(f"ğŸ¬ æ­£åœ¨ç”Ÿæˆè§†é¢‘ï¼Œè¾“å…¥å›¾åƒè·¯å¾„: {resolved}")

    input_image = load_image_pil(resolved)

    pipe = StableVideoDiffusionPipeline.from_pretrained(
        "stabilityai/stable-video-diffusion-img2vid-xt",
        torch_dtype=torch.float16,
        variant="fp16",
        cache_dir=str(MODEL_CACHE_DIR)
    )
    pipe.enable_model_cpu_offload()
    pipe.enable_attention_slicing()

    # ä¸€æ¬¡æ€§ç”Ÿæˆå…¨éƒ¨å¸§
    out = pipe(
        input_image,
        num_frames=total_frames,
        num_inference_steps=30,  # å¢åŠ æ­¥æ•°ï¼Œç»†èŠ‚æ›´ä¸°å¯Œ
        decode_chunk_size=4      # åˆ†å—è§£ç ï¼Œé˜²çˆ†æ˜¾å­˜
    )
    frames = out.frames[0]

    base_name = Path(resolved).stem
    save_frames(frames, base_name=base_name)
    save_video(frames, OUTPUT_VIDEO_DIR / f"{base_name}.mp4", fps=fps)

    print(f"âœ… å·²ç”Ÿæˆ {len(frames)} å¸§ï¼Œè§†é¢‘è¾“å‡ºè·¯å¾„ä¸º /input/video/{base_name}.mp4")

# ---------- æœ¬åœ°å…¥å£ ----------
if __name__ == "__main__":
    image_arg = sys.argv[1] if len(sys.argv) > 1 else None
    app.functions.generate_video.call(image_path=image_arg or "/input/input.png")

    print("ğŸ‰ è¿œç¨‹è§†é¢‘ç”Ÿæˆå®Œæˆï¼Œå¸§ & è§†é¢‘å·²ä¿å­˜åˆ° Volume svd-cacheã€‚")
    print("ğŸ‘‰ è‹¥è¦ä¸‹è½½ï¼š")
    print("   modal volume get svd-cache /video  D:/svd_output/video")
    print("   modal volume get svd-cache /frames D:/svd_output/frames")















